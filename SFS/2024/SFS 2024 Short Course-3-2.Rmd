---
title: "SFS 2024 Short Course -- Bayesian Applications in Environmental and Ecological
  Studies with R and Stan"
author: "Song S. Qian and Mark R. DuFour"
date: "6/1/2024"
output:
  pdf_document: default
  html_document:
    df_print: paged
urlcolor: blue
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
source("Front.R")
packages(lattice)
packages(arm)
```

## Hierarchical Structure and Regression Using Data from Multiple Sources
If we define "big data" as data obtained from multiple sources and representing multiple levels of aggregation, it becomes evident that most of the data we utilize in our work falls into the category of big data. In our context, the era of big data coincides with the era of hierarchical modeling. Failing to appropriately address the hierarchical structure inherent in the data can often result in misleading conclusions when working with big data.  

When applying a regression model to data from multiple sources, we can either fitting regression models separately for each source or combining data together to fit one regression model. The former assumes data from different sources are completely different from each other (no pooling) and the later assumes that different sources are identical and can be treated as replicates (complete pooling).  In practice, neither is satisfactory.  When analyzing data from similar studies carried out in different locations, we cannot, on the one hand, assume that data are replicates because of regional differences in natural conditions.  On the other hand, these studies targeted the same or similar relationships.  As a result, the data are not entirely different, nor the same as replicates.  When using the hierarchical modeling approach we explicitly incorporate the commonality and the potential differences in the same model to improve the overall optimal results.  

In this section, we use the data from a USGS National Water Quality Assessment (NAWQA) study to illustrate the Bayesian hierarchical modeling approach of a linear regression problem.  Here, we use data from a NAWQA study to illustrate our general approach in modeling:  exploratory data analysis, proposing tentative models and fitting them using existing models in R, revising the model until a satisfactory one is achieved, and implementing the final model using Stan.  

### Effects of Urbanization on Stream Ecosystems (EUSE)
The EUSE project was briefly introduced in Qian (2016):

> The U.S. Geological Survey (USGS) is responsible for monitoring the country's natural resources. In 1991, USGS started a program to develop long-term consistent and comparable information on water quality and factors affecting aquatic ecosystems.  The program is designed to understand the conditions of streams, rivers, and groundwater in the U.S. and their temporal trends.  The program, known as the National Water Quality Assessment program (NAWQA), has both long-term monitoring networks and short-term topical studies.  The project on the effects of urbanization on stream ecosystem (EUSE) is a topic study with an emphasis on the effects of various urbanization-induced changes in the landscape on water quality and aquatic ecosystem.  The project, started in 1999, consists of a series of studies with a common design to examine the regional effects of urbanization on aquatic biota (fish, invertebrates, and algae), water chemistry, and physical habitat in nine metropolitan areas in different environmental settings.  These studies are known as ``urban gradient studies'' because they were conducted on watersheds selected along urban gradients within their respective study areas.  These study areas are in the metropolitan areas of Atlanta, Georgia (ATL); Boston, Massachusetts (BOS); Birmingham, Alabama (BIR); Denver, Colorado (DEN); Dallas-Fort Worth, Texas (DFW); Milwaukee-Green Bay, Wisconsin (MGB); Portland, Oregon (POR); Raleigh, North Carolina (RAL); and Salt Lake City, Utah (SLC).

> During the initial stage of EUSE, researchers developed a multimetric urban intensity index (UII) to identify representative gradients of urbanization within relatively homogeneous environmental settings associated with each urban area.  Within each study area, 30 watersheds were selected to represent the urbanization gradient. These watersheds are of similar size and other natural characteristics, so that researchers can address several main questions:

- Do physical, chemical, and biological characteristics of streams respond to urban intensity? 
- What are the rates of such response?
- What are typical indicators of changes caused by urbanization?
- What are typical  characteristics of biological response to
  increased urban intensity?
- Do biological responses to urbanization vary by region?

> Although these questions are ecological and environmental in nature, statistics played an important role in analyzing the data collected in the subsequent years. 

One indicator, average tolerance of macroinvertebrate taxa (TOLr), is of particular interest to USGS (Cuffney et al., 2005). The tolerance of a taxon is a measure of whether a taxon can survive in a polluted environment.  In general, taxa with a higher tolerant score can be found in waters with poorer water quality.  As such, the average tolerance scores of the sampled taxa from a water can be used as a biological measure of water quality. Initial analysis suggested that TOLr is linearly associated with watershed urbanization index (USGS' National Urban Intensity Index, NUII).  

```{R euse,tidy=F, fig.width=5, fig.height=5, dev="tikz"}
rtol2 <- read.csv(file=paste(dataDIR, "rtolforMS.csv", sep="/"), 
                  header=T)

## environmental data
euse.env <- read.csv(paste(dataDIR, "EUSE_NAT_ENV.csv", sep="/"),
                     header=T)
names(euse.env)[13]<-"MAX.ELEV"
names(euse.env)[12]<-"MIN.ELEV"

AvePrec <- tapply(euse.env$AnnMeanP, euse.env$CITY, mean)
AveTemp <- tapply(euse.env$AnnMeanT, euse.env$CITY, mean)
AveElev <- tapply(euse.env$MEANELEV, euse.env$CITY, mean)
AveMaxT <- tapply(euse.env$AnnMaxT, euse.env$CITY, mean)
AveMaxP <- tapply(euse.env$AnnMaxP, euse.env$CITY, mean)
AveMinP <- tapply(euse.env$AnnMinP, euse.env$CITY, mean)
AvePdif <- tapply(euse.env$AnnMaxP-euse.env$AnnMinP, euse.env$CITY, mean)

city_ag<-read.csv(paste(dataDIR, "City_AG_Grassland.csv",
                        sep="/"), header=T, na.strings = ".")
city_ag<-cbind(city_ag[,1:2],city_ag[,3:5]/100)
city_ag[order(city_ag[ ,1]), ]
ag<-city_ag[order(city_ag[ ,1]), ][,5]
ag.cat <- as.numeric(ag>0.5)
site <- as.numeric(ordered(rtol2$city))
ag.full <- as.vector(ag.cat[site])
temp.full <- as.vector(AveTemp[site])

rtol2$nuii <- rtol2$nuii/100

xyplot(richtol~nuii|city, data=rtol2, 
       panel=function(x,y,...){
         panel.xyplot(x, y, ...)
         panel.lmline(x,y,...)}, 
       ylab="TOLr", xlab="NUII", aspect=1)
```

We start with a linear regression model.  

```{R euse linear, tidy=F, fig.width=6.25, fig.height=7}
## No pooling:
euse.lm1 <- lm(richtol ~ nuii*city, data=rtol2)
display(euse.lm1, 4)

lm.plots(euse.lm1)

display(euse.lm11 <- update(euse.lm1,  .~. -1-nuii), 4)
## Complete pooling
euse.lm2 <- lm(richtol~nuii, data=rtol2)
display(euse.lm2, 4)
```

### BHM using `lmer`

A hierarchical model:
$$
\begin{array}{rcl}
 y_{ij} & = & \beta_{0j}+\beta_{1j}x_{ij}+\epsilon_{ij}\\
  \begin{pmatrix}
\beta_{0j}\\ \beta_{1j} \end{pmatrix} & \sim & MVN \begin{bmatrix}\begin{pmatrix} \mu_{\beta_0}\\ \mu_{\beta_1}\end{pmatrix}, \Sigma_\beta \end{bmatrix}\\
\epsilon_{ij} & \sim & N(0, \sigma^2)
\end{array}
$$
Can be implemented using restricted MLE (using function `lmer` from package `lme4`:

```{R euse lmer,tidy=F, fig.width=3.25, fig.height=5, dev="tikz"}
euse.lmer1 <- lmer(richtol ~ nuii + (1+nuii|city), data=rtol2)
display(euse.lmer1, 4)

dotplot(ranef(euse.lmer1))
```

The common prior for model coefficients:

$$
\begin{pmatrix}
\beta_{0j}\\ \beta_{1j} \end{pmatrix}  \sim  MVN \begin{bmatrix}\begin{pmatrix} \mu_{\beta_0}\\ \mu_{\beta_1}\end{pmatrix}, \Sigma_\beta \end{bmatrix}
$$
represents the exchangeable assumption imposed on the model: we know that model coefficients (intercepts and slopes) are different from city to city (indexed by $j$). But we don't know how the coefficients vary by city, therefore, we assign the same prior distribution on all 9 models. The prior model coefficients ($\mu_{\beta_0}, \mu_{\beta_1}, \Sigma_\beta$) are estimated from the combined data. 


```{R euse compare, tidy=F, fig.width=4.5, fig.height=4, dev="tikz"}
## comparing three linear models

## No pooling
ls2.int<-as.data.frame(rbind(summary(euse.lm11)$coef[1:9, 1:2], 
                             summary(euse.lm2)$coef[1, 1:2]))
ls2.slp<-as.data.frame(rbind(summary(euse.lm11)$coef[10:18, 1:2],
                             summary(euse.lm2)$coef[2, 1:2]))

## partial pooling
lmer1.int<-as.data.frame(rbind(cbind(ranef(euse.lmer1)[[1]][,1]+
                                       fixef(euse.lmer1)[1],
                               se.ranef(euse.lmer1)[[1]][,1]),
                               c(fixef(euse.lmer1)[1], 
                                 se.fixef(euse.lmer1)[1])))
lmer1.slp<-as.data.frame(rbind(cbind(ranef(euse.lmer1)[[1]][,2]+
                                       fixef(euse.lmer1)[2],  
                                 se.ranef(euse.lmer1)[[1]][,2]),
                               c(fixef(euse.lmer1)[2], 
                                 se.fixef(euse.lmer1)[2])))

par(mfrow=c(1,2), mar=c(4,4,0.5, 0.75), mgp=c(1.25,0.125,0),
    tck=0.01)
line.plots.compare(ls2.int[,1], ls2.int[,2], lmer1.int[,1],
                   lmer1.int[,2], 
                   c(levels(factor(rtol2$city)), "All"),
                   "Intercepts", 2)
box()
par(mar=c(4,0.75,0.5,4))
line.plots.compare(ls2.slp[,1], ls2.slp[,2], lmer1.slp[,1], lmer1.slp[,2], c(levels(factor(rtol2$city)), "All"), "Slopes", 4)
box()
## the shrinkage effect of BHM
```

### BHM with Group-level Predictor(s)

Why the differences?

The obvious difference among the 9 cities is the annual average temperature. 

Let $T_j$ be the annual average temperature of city $j$, we start with linear model:
$$
\begin{pmatrix}
\beta_{0j}\\ \beta_{1j} \end{pmatrix}  \sim  MVN \begin{bmatrix}\begin{pmatrix} a_0+a_1\times T_j\\ b_0+b_1\times T_j\end{pmatrix}, \Sigma_\beta \end{bmatrix}
$$

```{R euse group-level, tidy=F, width=5.75, height=4, dev="tikz"}
euse.lmer2 <- lmer(richtol ~ nuii + temp.full + nuii:temp.full +
                     (1+nuii|city), data=rtol2)
M2.coef <- coef (euse.lmer2)
a.hat.M2 <- M2.coef[[1]][,1] + M2.coef[[1]][,3]*AveTemp
b.hat.M2 <- M2.coef[[1]][,2] + M2.coef[[1]][,4]*AveTemp
a.se.M2 <- se.ranef(euse.lmer2)[[1]][,1]
b.se.M2 <- se.ranef(euse.lmer2)[[1]][,2]

# plot estimated intercepts and slopes
par (mfrow=c(1,2), mar=c(3,3,3,0.25), mgp=c(1.25,0.125,0), 
     las=1, tck=0.01)
lower <- a.hat.M2 - a.se.M2
upper <- a.hat.M2 + a.se.M2
plot (AveTemp, a.hat.M2, ylim=range(lower,upper), 
      cex.lab=0.75, cex.axis=0.75,
      xlab="average temperature", 
      ylab="regression intercept", pch=20)
curve (fixef(euse.lmer2)["(Intercept)"] +
         fixef(euse.lmer2)["temp.full"]*x, lwd=1, 
       col="black", add=TRUE)
segments (AveTemp, lower, AveTemp, upper, lwd=.5, col="gray10")
text(AveTemp, lower, levels(rtol2$city), adj=c(.5,1),cex=0.5)

lower <- b.hat.M2 - b.se.M2
upper <- b.hat.M2 + b.se.M2

plot (AveTemp, b.hat.M2, ylim=range(lower,upper), cex.lab=0.75,
      cex.axis=0.75,
      xlab="average temperature", ylab="regression slope",
      pch=20)
curve (fixef(euse.lmer2)["nuii"] +
         fixef(euse.lmer2)["nuii:temp.full"]*x, lwd=1,
       col="black", add=TRUE)
segments (AveTemp, lower, AveTemp, upper, lwd=.5, col="gray10")
text(AveTemp, lower, levels(rtol2$city), adj=c(.5,1),cex=0.5)
```

The 9 cities can be grouped into 2 groups, grouped by background agriculture land use. Now let $Ag_j$ be the historical agricultural land use percentage:
$$
\begin{pmatrix}
\beta_{0j}\\ \beta_{1j} \end{pmatrix}  \sim  MVN \begin{bmatrix}\begin{pmatrix} a_0+a_1\times Ag_j\\ b_0+b_1\times Ag_j\end{pmatrix}, \Sigma_\beta \end{bmatrix}
$$

```{R euse group-level 2, tidy=F, width=5.75, height=4, dev="tikz"}
euse.lmer3 <- lmer(richtol ~ nuii+ag.full+nuii:ag.full+
                     (1+nuii|site), data=rtol2)
display(euse.lmer3, 4)  ## display no plonger works for lmer

M3.coef <- coef (euse.lmer3)
a.hat.M3 <- M3.coef[[1]][,1] + M3.coef[[1]][,3]*ag
b.hat.M3 <- M3.coef[[1]][,2] + M3.coef[[1]][,4]*ag
a.se.M3 <- se.ranef(euse.lmer3)[[1]][,1]
b.se.M3 <- se.ranef(euse.lmer3)[[1]][,2]

# plot estimated intercepts and slopes

par (mfrow=c(1,2), mar=c(3,3,3,0.25), mgp=c(1.25,0.125,0), 
     las=1, tck=0.01)
lower <- a.hat.M3 - a.se.M3
upper <- a.hat.M3 + a.se.M3
plot (ag, a.hat.M3, ylim=range(lower,upper), cex.lab=0.75,
      cex.axis=0.75,
      xlab="Background ag", ylab="regression intercept",
      pch=20)
curve (fixef(euse.lmer3)["(Intercept)"] +
         fixef(euse.lmer3)["ag.full"]*x, lwd=1, col="black",
       add=TRUE)
segments (ag, lower, ag, upper, lwd=.5, col="gray10")
text(ag, lower, levels(rtol2$city), adj=c(.5,1),cex=0.5)

lower <- b.hat.M3 - b.se.M3
upper <- b.hat.M3 + b.se.M3

plot (ag, b.hat.M3, ylim=range(lower,upper), cex.lab=0.75,
      cex.axis=0.75, xlab="Background ag", 
      ylab="regression slope", pch=20)
curve (fixef(euse.lmer3)["nuii"] + 
         fixef(euse.lmer3)["nuii:ag.full"]*x, 
       lwd=1, col="black", add=TRUE)
segments (ag, lower, ag, upper, lwd=.5, col="gray10")
text(ag, lower, levels(rtol2$city), adj=c(.5,1),cex=0.5)
```

Adding both `ag` and `temp.full`: 
$$
\begin{pmatrix}
\beta_{0j}\\ \beta_{1j} \end{pmatrix}  \sim  MVN \begin{bmatrix}\begin{pmatrix} a_0+a_1\times T_j +a_2\times Ag_j\\ b_0+b_1\times T_j+b_2\times Ag_j\end{pmatrix}, \Sigma_\beta \end{bmatrix}
$$

```{R euse group-level 3, tidy=F, width=5.75, height=4, dev="tikz"}
ag.cat2 <- ag.full>0.5

euse.lmer4 <- lmer(richtol ~ nuii+temp.full+nuii:temp.full+
                     (1+nuii|site)+
                     (1+nuii|ag.cat2), data=rtol2)

display(euse.lmer4, 4)

M4.fixef <- fixef (euse.lmer4)
M4.ranef <- ranef (euse.lmer4)
M4.sefixef <- se.fixef (euse.lmer4)
M4.seranef <- se.ranef (euse.lmer4)

a.hat.M4 <- M4.fixef[1] + M4.ranef[[1]][,1] +
  M4.ranef[[2]][c(1,1,1,2,2,2,1,1,1),1] + 
  M4.fixef[3]*AveTemp
b.hat.M4 <- M4.fixef[2] + M4.ranef[[1]][,2] +
  M4.ranef[[2]][c(1,1,1,2,2,2,1,1,1),2] + 
  M4.fixef[4]*AveTemp

a.se.M4 <- M4.seranef[[1]][,1]
b.se.M4 <- M4.seranef[[1]][,2]

# plot estimated intercepts and slopes

par (mfrow=c(1,2), mar=c(3,3,3,0.25), mgp=c(1.25,0.125,0),
     las=1, tck=0.01)
lower <- a.hat.M4 - a.se.M4
upper <- a.hat.M4 + a.se.M4
plot (AveTemp, a.hat.M4, ylim=range(lower,upper), 
      cex.lab=0.75,  cex.axis=0.75, cex=0.5,
      xlab="Ave Temp", ylab="regression intercept", pch=20)
curve (fixef(euse.lmer4)[1] + fixef(euse.lmer4)[3]*x +
         M4.ranef[[2]][1,1], lwd=1, col="black", add=TRUE)
curve (fixef(euse.lmer4)[1] + fixef(euse.lmer4)[3]*x +
         M4.ranef[[2]][2,1], lwd=1, col="black", add=TRUE)
segments (AveTemp, lower, AveTemp, upper, lwd=.5, col="gray10")
text(AveTemp, lower, levels(rtol2$city), adj=c(.5,1),cex=0.5)

lower <- b.hat.M4 - b.se.M4
upper <- b.hat.M4 + b.se.M4

plot (AveTemp, b.hat.M4, ylim=range(lower,upper), cex.lab=0.75,
      cex.axis=0.75, cex=0.5,
      xlab="Ave Temp", ylab="regression slope", pch=20)
curve (fixef(euse.lmer4)[2] + fixef(euse.lmer4)[4]*x +
         M4.ranef[[2]][1,2], lwd=1, col="black", add=TRUE)
curve (fixef(euse.lmer4)[2] + fixef(euse.lmer4)[4]*x +
         M4.ranef[[2]][2,2], lwd=1, col="black", add=TRUE)
segments (AveTemp, lower, AveTemp, upper, lwd=.5, col="gray10")
text(AveTemp, lower, levels(rtol2$city), adj=c(.5,1),cex=0.5)
```

### BHM using Stan

Model `euse.lmer4` is the best model -- 

But the algorithm used in `lme4` has difficulty in estimating the variances in the model. Using Stan, we need to have an efficient way to model the multivariate normal distribution, especially the covariance matrix $\Sigma_\beta$.  The conjugate prior for the covariance matrix is the inverse Wishart distribution.

Stan reference manual recommends using a specification of the covariance matrix that is computationally suitable for Stan.  The covariance matrix is decomposed into a vector of scales (standard deviation) and a correlation matrix:

$$
\Sigma_\beta= \mathrm{diag\_matrix}(\tau)\times\Omega\times\mathrm{diag\_matrix}(\tau)
$$
where $\Omega$ is a correlation matrix and $\tau$ is the coefficient scale matrix (standard deviation of $\beta_0$ and $\beta_1$). Stan User's Manual recommends a half-Cauchy distribution as the prior for $\tau$, specifically, $\tau \sim \mathrm{Cauchy}(0, 2.5)$. The correlation matrix $\Omega$ is recommended using the LJK prior with a shape $>1$. When written in Stan code, the variance-covariance matrix part can be expressed as follows:
```
...
parameters{
  vector[K] beta[Nreg];
  corr_matrix[K] Omega;
  row_vector[K] mu_b;
  vector[K] tau;
...
model{
  tau ~ cauchy(0,2.5);
  Omega ~ ljk_corr(2);
  beta ~ multi_normal(mu_b, quad_form_diag(Omega,tau));
}
...
```

To improve computational efficiency, we can parameterize the correlation matrix $\Omega$ in terms of its Cholesky factor $\mathbf{L}$ (i.e., $\mathbf{L}\mathbf{L}'=\Omega$)using Stan's Cholesky LJK correlation distribution. As in a univariate normal random variable $y\sim N(\mu,\sigma^2)$ with known $\mu$ and $\sigma$ where we can draw random numbers of $y$ by drawing a standard normal random variable ($z\sim N(0,1)$) and calculate $y=\mu+z\sigma$, a multivariate normal vector $\mathbf{y} \sim MVN(\mathbf{\mu},\Sigma)$ can be drawn by using $z$: $\mathbf{y} = \mathbf{\mu}+z\times \sqrt{\Sigma}$. Because $\mathbf{L}$ is the square root of the correlation matrix, the covariance matrix $\Sigma = (\mathrm{diag\_matrix}(\tau)\cdot \mathbf{L})(\mathbf{L}\cdot \mathrm{diag\_matrix}(\tau))^T$. In Stan, such computation is made simple with the following distribution and function:
```
parameter{
  matrix[K,Nreg] z;
  cholesky_factor_corr[K] L_Omega;
  row_vector[K] mu_b;
  vector<lower=0>[K] tau;
}
transformed parameters{
  matrix[Nreg,K] beta;
  beta = rep_matrix(mu_b,Nreg)+
         (diag_pre_multiply(tau,L_Omega)*z)'; 
}
model {
  //prior
  to_vector(z) ~ std_normal();
  L_Omega ~ ljk_corr_cholesky(2);
  //likelihood
  ...
}
```

#### The Stan model without group-level predictors

We demonstrate Stan implementations of linear BHM.

```{R euse stan 1, tidy=F, dev="tikz"}
euse_stan1 <- "
data {
  int<lower=0> N;
  int<lower=0> Nreg;
  int<lower=1> K;
  int<lower=1,upper=Nreg> region[N];
  matrix[N,K] x;
  vector[N] y;
}
parameters {
  vector[K] beta[Nreg];
  corr_matrix[K] Omega;
  row_vector[K] mu_b;
  vector<lower=0>[K] tau;
  real<lower=0,upper=10> sigma_y;
}
transformed parameters {
  vector[N] mu_y;
  for (i in 1:N)
    mu_y[i] = x[i] * beta[region[i]];
}
model {
  tau ~ cauchy(0,2.5);
  Omega ~ lkj_corr(2);
  beta ~ multi_normal(mu_b, quad_form_diag(Omega, tau));
  y ~ normal(mu_y, sigma_y);
}
"
fit1 <- stan_model(model_code=euse_stan1)
stan_in1 <- function(data=rtol2, chains=nchains){
    y <- data$richtol
    x <- as.matrix(cbind(1, data$nuii))
    n <- dim(data)[1]
    k <- dim(x)[2]
    reg <- as.numeric(ordered(data$city))
    nreg <- max(reg)
    stan_data <- list(N=n, Nreg=nreg, K=k, x=x, y=y, region=reg)
    stan_inits <- list()
    for (i in 1:chains)
        stan_inits[[i]] <- list(beta=matrix(rnorm(nreg*k),
                                            nrow=nreg),
                                mu_b=rnorm(k), tau=runif(k),
                                sigma_y=runif(1))
    stan_pars <- c("beta","mu_b","tau","sigma_y")
    return(list(data=stan_data, inits=stan_inits,
                pars=stan_pars, n.chains=chains))
}

input.to.stan <- stan_in1()
fit2keep <- sampling(fit1, data=input.to.stan$data,
                     init=input.to.stan$inits,
                     pars=input.to.stan$pars,
                     iter=niters,thin=nthin,
                     chains=input.to.stan$n.chains)#,
#                     control=list(adapt_delta = 0.9))
print(fit2keep)
```

Now processing Stan output using `rv`

```{R euse stan 2, tidy=F, dev="tikz"}
## EUSE no group-level predictor
euse_multilevel1 <- rvsims(as.matrix(as.data.frame(
    rstan::extract(fit2keep, pars=c("beta","mu_b",
                   "tau", "sigma_y")))))

## compare to lmer results
### 1. Intercepts
cbind(coef(euse.lmer1)$city[,1],
summary(euse_multilevel1[1:9])$mean)

### 2. Slopes
cbind(coef(euse.lmer1)$city[,2],
summary(euse_multilevel1[10:18])$mean)

### 3. fixed effect standard deviation
cbind(fixef(euse.lmer1),
summary(euse_multilevel1[19:20])$mean)

### 4. Variance components
cbind(attr(summary(euse.lmer1)$varcor$city, "stddev"), 
summary(euse_multilevel1[21:22])$mean)

### 5 Residual stddev:
c(attr(summary(euse.lmer1)$varcor, "sc"),summary(euse_multilevel1[23])$mean) 
```

The same model can be programmed using Cholesky factorization:

```{R euse Stan 3, tidy=F}
euse_stan15 <- "
data {
  int<lower=0> N;
  int<lower=0> Nreg;
  int<lower=1> K;
  int<lower=1,upper=Nreg> region[N];
  matrix[N,K] x;
  vector[N] y;
}
parameters {
  matrix[K,Nreg] z;
  cholesky_factor_corr[K] L_Omega;
  row_vector[K] mu_b;
  vector<lower=0>[K] tau;
  real<lower=0,upper=10> sigma_y;
}
transformed parameters {
  matrix[Nreg,K] beta;
  beta = rep_matrix(mu_b, Nreg) +
               (diag_pre_multiply(tau,L_Omega)*z)';
}
model {
  to_vector(z) ~ std_normal();
  L_Omega ~ lkj_corr_cholesky(2);
  y ~ normal(rows_dot_product(beta[region], x), sigma_y);
}
"
fit15 <- stan_model(model_code=euse_stan15)
stan_in15 <- function(data=rtol2, chains=nchains){
    y <- data$richtol
    x <- as.matrix(cbind(1, data$nuii))
    n <- dim(data)[1]
    k <- dim(x)[2]
    reg <- as.numeric(ordered(data$city))
    nreg <- max(reg)
    stan_data <- list(N=n, Nreg=nreg, K=k, x=x, y=y, region=reg)
    stan_inits <- list()
    for (i in 1:chains)
        stan_inits[[i]] <- list(mu_b=rnorm(k), tau=runif(k),
                                sigma_y=runif(1))
    stan_pars <- c("beta","mu_b","tau","sigma_y")
    return(list(data=stan_data, inits=stan_inits,
                pars=stan_pars, n.chains=chains))
}

input.to.stan <- stan_in15()
fit2keep <- sampling(fit15, data=input.to.stan$data,
                     init=input.to.stan$inits,
                     pars=input.to.stan$pars,
                     iter=niters,thin=nthin,
                     chains=input.to.stan$n.chains)
print(fit2keep)
euse_multilevel15 <- rvsims(as.matrix(as.data.frame(
    rstan::extract(fit2keep,
                   pars=c("beta","mu_b","tau","sigma_y")))))
```

#### Stan model with group-level predictors

We now add `temp` and `ag` as group-level predictors (the "best" model `euse.lmer4`). The code below is written in matrix notation. The model and code consider the potential of an interaction term between $ag$ and $T$:
$$
\begin{pmatrix}
\beta_{0j}\\ \beta_{1j} \end{pmatrix}  \sim  MVN \begin{bmatrix}\begin{pmatrix} a_0+a_1\times T_j +a_2\times Ag_j + a_3Ag_j\times T_j\\ b_0+b_1\times T_j+b_2\times Ag_j+ b_3Ag_j\times T_j\end{pmatrix}, \Sigma_\beta \end{bmatrix}
$$
However, in the matrix notation, the above equation is simply 
$$
\mathbf{\beta}_j = MVN(\mathbf{gr_{pred}}_j\times \mathbf{\gamma},\Sigma_\beta)
$$
where $\mathbf{\beta}_j$ is the $j$th row of the regression coefficients for region $j$ (there are 9 regions each with 2 regression coefficients, intercept $\beta_0$ and slope $\beta_1$), 
$$
\mathbf{\gamma} = \begin{pmatrix} a_0 & b_0 \\ a_1&b_1\\ a_2&b_2\\a_3&b_3 \end{pmatrix}
$$ 
is the group-level model coefficient matrix, and 
$$
\mathbf{gr_{pred}}_j=\begin{pmatrix}1&T_1&Ag_1&T_1\times Ag_1\\ \cdots\\1&T_9&Ag_9&T_9\times Ag_9\end{pmatrix}
$$ 

is the design matrix of the group-level predictor values for region $j$.

```{R euse Stan 4, tidy=F}
euse_stan2 <- "
data {
  int<lower=0> N;
  int<lower=0> Nreg;
  int<lower=1> K; // individual predictor
  int<lower=1> J; // group-level predictor
  int<lower=1,upper=Nreg> region[N];
  matrix[N,K] x;
  vector[N] y;
  row_vector[J] gr[Nreg];
}
parameters {
  vector[K] beta[Nreg];
  matrix[J,K] gamma;
  corr_matrix[K] Omega;
  vector<lower=0>[K] tau;
  real<lower=0,upper=10> sigma_y;
}
transformed parameters {
  vector[N] mu_y;
  row_vector[K] u_gamma[Nreg];
  for (j in 1:Nreg)
    u_gamma[j] = gr[j] * gamma;
  for (i in 1:N)
    mu_y[i] = x[i] * beta[region[i]];
}
model {
  tau ~ cauchy(0,2.5);
  Omega ~ lkj_corr(2);
  beta ~ multi_normal(u_gamma, quad_form_diag(Omega, tau));
  y ~ normal(mu_y, sigma_y);
}
"

fit2 <- stan_model(model_code=euse_stan2)

stan_in2 <- function(data=rtol2, Ag=ag.cat, Temp=AveTemp,
                     y="richtol", x="nuii", regn="city",
                     Log=F, int=F, chains=nchains){
  ## The default is not including the interaction
    if (Log){
        y <- log(data[,y])
        x <- log(data[,x])
    } else {
        y <- data[,y]
        x <- data[,x]
    }
    x <- as.matrix(cbind(1, x))
    n <- dim(data)[1]
    k <- dim(x)[2]
    reg <- as.numeric(ordered(data[,regn]))
    nreg <- max(reg)
    if (int) regPred <- cbind(1, 2*(Ag-0.5), Temp-mean(Temp),
                              2*(Ag-0.5)*Temp-mean(Temp))
    else
        regPred <- cbind(1, 2*(Ag-0.5), Temp-mean(Temp))
    nregpred <- dim(regPred)[2]
    stan_data <- list(N=n, Nreg=nreg, K=k, J=nregpred, x=x, y=y,
                      region=reg, gr = regPred)
    stan_inits <- list()
    for (i in 1:chains)
        stan_inits[[i]] <- list(beta=matrix(rnorm(nreg*k),
                                            ncol=k),
                                gamma=matrix(rnorm(nregpred*k),
                                             ncol=k),
                                tau=runif(k), sigma_y=runif(1))
    stan_pars <- c("beta","gamma","tau","sigma_y")
    return(list(data=stan_data, inits=stan_inits,
                pars=stan_pars, n.chains=chains))
}

input.to.stan <- stan_in2() ## no interaction
fit2keep <- sampling(fit2, data=input.to.stan$data,
                     init=input.to.stan$inits,
                     pars=input.to.stan$pars,
                     iter=niters, thin=nthin,
                     chains=input.to.stan$n.chains,
                     control=list(adapt_delta = 0.95,
                                  max_treedepth=15))

print(fit2keep)
pairs(fit2keep, pars=c("gamma","tau"))

euse_multilevel2 <- rvsims(as.matrix(as.data.frame(
    rstan::extract(fit2keep, pars=c("beta","gamma",
                   "tau", "sigma_y")))))
```

We can also use Cholesky decomposition:

```{R euse Stan 5, tidy=F}
## Cholesky decomposition
euse_stan3 <- "
data {
  int<lower=0> N;
  int<lower=0> Nreg;
  int<lower=1> K; // individual predictor
  int<lower=1> J; // group-level predictor
  int<lower=1,upper=Nreg> region[N];
  matrix[N,K] x;
  vector[N] y;
  matrix[Nreg,J] gr;
}
parameters {
  matrix[K,Nreg] z;
  cholesky_factor_corr[K] L_Omega;
  vector<lower=0,upper=pi()/2>[K] tau_unif;
  matrix[J,K] gamma;
  real<lower=0,upper=10> sigma_y;
}
transformed parameters {
  matrix[Nreg,K] beta;
  vector<lower=0>[K] tau;

  for (k in 1:K)
    tau[k] = 2.5*tan(tau_unif[k]);
  beta = gr*gamma + (diag_pre_multiply(tau,L_Omega)*z)';
}
model {
  to_vector(z) ~ std_normal();
  L_Omega ~ lkj_corr_cholesky(2);
  to_vector(gamma) ~ normal(0,5);
  y ~ normal(rows_dot_product(beta[region], x), sigma_y);
}
"
fit3 <- stan_model(model_code=euse_stan3)
stan_in3 <- function(data=rtol2, Ag=ag.cat, Temp=AveTemp,
                     y="richtol", x="nuii", regn="city",
                     Log=F, int=F, chains=nchains){
    if (Log){
        y <- log(data[,y])
        x <- log(data[,x])
    } else {
        y <- data[,y]
        x <- data[,x]
    }
    x <- as.matrix(cbind(1, x))
    n <- dim(data)[1]
    k <- dim(x)[2]
    reg <- as.numeric(ordered(data[,regn]))
    nreg <- max(reg)
    if (int)
        regPred <- cbind(1, 2*(Ag-0.5), Temp-mean(Temp),
                         2*(Ag-0.5)*(Temp-mean(Temp)))
    else regPred <- cbind(1, 2*(Ag-0.5), Temp-mean(Temp))
    nregpred <- dim(regPred)[2]
    stan_data <- list(N=n, Nreg=nreg, K=k, J=nregpred, x=x, y=y,
                      region=reg, gr = regPred)
    stan_inits <- list()
    for (i in 1:chains)
        stan_inits[[i]] <- list(z=matrix(rnorm(k*nreg), nrow=k),
                                gamma=matrix(rnorm(nregpred*k),
                                             ncol=k),
                                sigma_y=runif(1))
    stan_pars <- c("beta","gamma","tau","sigma_y")
    return(list(data=stan_data, inits=stan_inits,
                pars=stan_pars, n.chains=chains))
}

input.to.stan <- stan_in3()
fit2keep <- sampling(fit3, data=input.to.stan$data,
                     init=input.to.stan$inits,
                     pars=input.to.stan$pars,
                     iter=niters,thin=nthin,
                     chains=input.to.stan$n.chains,
                     control=list(adapt_delta = 0.9))

print(fit2keep)

euse_multilevel3 <- rvsims(as.matrix(as.data.frame(
    rstan::extract(fit2keep, pars=c("beta","gamma",
                   "tau", "sigma_y")))))
```

We don't have warnings for this model. We now use the output to generate the same plot we see before:

```{R euse Stan figures, tidy=F, height=3, width=5, dev="tikz"}
beta_rv <- summary(rvmatrix(euse_multilevel3[1:18], nrow=9))
gamma_rv <- summary(rvmatrix(euse_multilevel3[19:24], nrow=3))
gr <- input.to.stan$data$gr

par(mfrow=c(1,2), mar=c(3, 3, 1, 0.25), mgp=c(1.5,0.125,0), las=1, tck=0.01)
plot(AveTemp,beta_rv[1:9, 1], xlab="average temperature",
     ylab="regression intercept", ylim=c(4,7.25))
a <- sum(gamma_rv[1:2,1]*c(1,-1)) - gamma_rv[3,1]*mean(AveTemp)
abline(a, gamma_rv[3,1])
a <- sum(gamma_rv[1:2,1]*c(1,1)) - gamma_rv[3,1]*mean(AveTemp)
abline(a, gamma_rv[3,1])
segments(y0=beta_rv[1:9, 4], y1=beta_rv[1:9, 8], x0=AveTemp, x1=AveTemp)

plot(AveTemp,beta_rv[10:18, 1], xlab="average temperature",
     ylab="regression slope", ylim=c(-0.01, 0.05)*100)
a <- sum(gamma_rv[4:5,1]*c(1,-1)) - gamma_rv[6,1]*mean(AveTemp)
abline(a, gamma_rv[6,1])
a <- sum(gamma_rv[4:5,1]*c(1,1)) - gamma_rv[6,1]*mean(AveTemp)
abline(a, gamma_rv[6,1])
segments(y0=beta_rv[10:18, 4], y1=beta_rv[10:18, 8],
         x0=AveTemp, x1=AveTemp)
```

The effort of using Stan (all the above code) is substantially larger than using `lmer`.  This is why we often don't use Stan until we are satisfied with the final choice of the model.  We then implement the final model in Stan. Let's compare the results from the two methods.  The comparison is harder than the comparison of the model without group-level predictors. 

```{R euse Stan compare, tidy=F}
## fixed effects:
fixef(euse.lmer4) ## 4 coefficients:
## y = beta0+beta1*nuii + beta2* temp + beta3*(nuii*temp)
##or =(beta0+beta2*temp) + (beta1+beta3*temp)*nuii
## now compared to the Stan model output:

euse_multilevel3[1:18]
## the regional coefficients after Temp and Ag already considered

fixed <-fixef(euse.lmer4)
se.fixed <- se.fixef(euse.lmer4)
rndm <- ranef(euse.lmer4)
se.rndm <- se.ranef(euse.lmer4)
int <- fixed[1]+fixed[3]*AveTemp + rndm$ag.cat2[ag.cat+1,1]
## but the uncertainty cannot easily estimated 
se.int <- sqrt(se.fixed[1]^2+se.fixed[3]^2*AveTemp^2 +
               rndm$ag.cat2[ag.cat+1,1]^2) 
## assuming independent terms (not correct)

slp <- fixed[2]+fixed[4]*AveTemp + rndm$ag.cat2[ag.cat+1,2]
se.slp <- sqrt(se.fixed[2]^2+se.fixed[4]^2*AveTemp^2 + 
               se.rndm$ag.cat2[ag.cat+1,2]^2)

## slopes:
cbind(slp, summary(euse_multilevel3[10:18])$mean)
cbind(se.slp, summary(euse_multilevel3[10:18])$sd)

## intercepts
cbind(int, summary(euse_multilevel3[1:9])$mean)
cbind(se.int, summary(euse_multilevel3[1:9])$sd)

se.fixef(euse.lmer4)
se.ranef(euse.lmer4)
```
