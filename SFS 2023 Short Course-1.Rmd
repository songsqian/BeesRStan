---
title: "SFS 2023 Short Course -- Bayesian Applications in Environmental and Ecological Studies with R and Stan"
author: "Song S. Qian"
date: "6/3/2023"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
source("FrontMatter.R")

packages(rstan)
packages(rv)
packages(car)
rstan_options(auto_write = TRUE)
options(mc.cores = min(c(parallel::detectCores(), 8)))

nchains <-  min(c(parallel::detectCores(), 8))
niters <- 5000
nkeep <- 2500
nthin <- ceiling((niters/2)*nchains/nkeep)

```
## Introduction

- Statistical inference mode -- hypothetical deduction implemented through three basic "problems" (Fisher 1921)
  
  1. Problem of formulation -- what is the distribution of the response variable
  2. Problem of estimation -- how to estimate parameters of the model (from problem 1)
  3. Problem of distribution -- how to evaluate the model once we have the data

- A mathematical summary of statistical inference
  
  - The probability distribution density function of the response variable $y$: $\pi(y, \theta)$
  - Observing data $y_1,y_2\cdots,y_n$, we want to know the joint distribution of $\pi(y_1,y_2,\cdots,y_n, \theta)$. True for both classical and Bayesian inference.
  - Classical statistics -- $\pi(y_1,\cdots,y_n, \theta) = \pi(y_1,\cdots,y_n\mid \theta)\pi(\theta)$ 
  - The likelihood function: $L(\theta; y)=\pi(y_1,\cdots,y_n\mid \theta)$ and MLE
  - Sampling distribution -- uncertainty
  - Bayesian statistics --  
    - $\pi(y_1,\cdots,y_n, \theta) = \pi(y_1,\cdots,y_n\mid \theta)\pi(\theta)=\pi(\theta\mid y_1,\cdots,y_n)\pi(y_1,\cdots,y_n)$
    - Posterior distribution $\pi(\theta\mid y_1,\cdots,y_n)=\frac{\pi(y_1,\cdots,y_n\mid \theta)\pi(\theta)}{\pi(y_1,\cdots,y_n)}\propto \pi(y_1,\cdots,y_n\mid \theta)\pi(\theta)$ 
    - Combining estimation and distribution 
- Bayesian inference is a generalization of MLE-based classical inference: MLE is the mode of posterior distribution with uniform prior

### From Mathematics to Computation
- Derivative (classical) versus integration (Bayesian)
  - MLE: find parameter values that maximize the likelihood function $\hat{\theta}=arg\ max_{\theta\in\Theta} L(\theta;y)$
  - Bayesian: $\pi(\theta\mid y_1,\cdots,y_n)=\frac{\pi(y_1,\cdots,y_n\mid \theta)\pi(\theta)}{\int\pi(y_1,\cdots,y_n\mid \theta)\pi(\theta)d\theta}$
- Classical statistics: a collection of efficient numerical algorithms for quantifying MLEs from a number of classes of models
  - Historically, tabulated results
  - Now, implemented in software such as SAS, SPSS, and R
- Bayesian statistics: Using Monte Carlo simulation to avoid integration
  - Markov chain Monte Carlo simulation (MCMC) algorithm -- drawing random numbers of $\theta$ from its density function (or a function proportional to its density function)
  - Posterior distributions of model parameters -- evaluated using random samples from their posterior distributions
  - The Bayesian posterior is proportional to $L(\theta;y_1,\cdots,y_n)\pi(\theta)$
  - If $y_1,\cdots,y_n$ i.i.d., $L(\theta;y_1,\cdots,y_n)=\prod_{i=1}^n  L(\theta;y_i)$ and
  - The posterior 
  $$\log \left (\pi(\theta\mid y_1,\cdots,y_n) \right ) = \log \left (\pi(\theta)\right )+\sum_{i=1}^n\log \left (L(\theta;y_i)\right )$$
- `Stan` via `rstan`in R:
  - A program implements MCMC when $\log \left(\pi(\theta) \right )$ and $\log \left (L(\theta;y_i) \right )$ are provided.

### Example -- Snake Fungal Disease
- Estimating the prevalence of fungal infection in a population of protected snake species in Michigan
  - qPCR test with false positive rate (probability) $f_p$ and false negative rate $f_n$.
  - In a sample of $n$ snakes, $y$ tested positive
  - What is the prevalence of the disease in the population ($\theta$)
- Response variable $y$ modeled by the binomial distribution
  $$
  y \sim Bin(p, n)
  $$
  where $p$ is the probability of testing positive, and $p = \theta(1-f_n)+(1-\theta)f_p$
- The likelihood function $L=p^y(1-p)^{n-y}$
- Log likelihood: $\log(L) = y\log(p) +(n-y)\log(1-p)$
- Using a beta distribution prior for $\theta$: $\pi(\theta) \propto \theta^\alpha(1-\theta)^{\beta}$
- The log posterior density $\pi(\theta\mid y,n) \propto \alpha\log(\theta)+\beta\log(1-\theta)+ y\log(p) +(n-y)\log(1-p)$
- Computational options:
  - Classical statistics: $\hat{p} = y/n$, $\hat{\theta}=\frac{\hat{p}-f_p}{1-(f_p+f_n)}$ (because MLE is transformation invariant). Uncertainty: variance of $\hat{p}\approx (1-\hat{p})\hat{p}/n$, variance of $\hat{\theta}$ is $(1/(1-f_p-f_n))^2(1-\hat{p})\hat{p}/n$

```{R}
n <- 20
y <- 5
fn <- 0.05
fp <- 0.07
p_hat <- y/n
p_hat_sd <- sqrt(p_hat*(1-p_hat)/n)
theta_hat <- (p_hat - fp)/(1-fp-fn)
theta_hat_sd <- p_hat_sd/(1-fp-fn)
```
  - Bayesian 1: brute force numerical integration
  Calculating the posterior density over a grid between 0 and 1 and normalize the results.
```{R}
## prior beta(1,1)
  post_impft <- function(x=5, n=20, fp=0.07, fn=0.05, k=100){
    theta <- seq(0, 1,, k)
    fpst <- theta*(1-fn) + (1-theta)*fp
    post <- x*log(fpst) + (n-x)*log(1-fpst)
    return(list(pdf=exp(post)/(theta[2]*sum(exp(post))), cdf=cumsum(exp(post))/sum(exp(post))))
}

k <- 100
post_theta <- post_impft(k=k)
par(mar=c(3, 3, 1, 0.5), mgp=c(1.25, 0.125, 0), las=1, tck=0.01)
plot(seq(0, 1,, 100), post_theta$pdf, type="l", xlab="$\\theta$",
     ylab="$\\pi(\\theta|y)$")

## the mean: expected value
theta_seq <- seq(0,1,,100)
theta_mean <- theta_seq[2]*sum(post_theta$pdf * seq(0,1,,100))
theta_sd <- sqrt(sum(theta_seq[2]*post_theta$pdf*(seq(0,1,,100)-theta_mean)^2))

## mode = MLE
theta_mod <- theta_seq[post_theta$pdf==max(post_theta$pdf)]

## median
theta_med <- theta_seq[post_theta$cdf-0.5==min(abs(post_theta$cdf-0.5))]

## 5%- and 95% -tiles
theta_CI <- c(theta_seq[post_theta$cdf-0.05==min(abs(post_theta$cdf-0.05))], 
              theta_seq[post_theta$cdf-0.95==min(abs(post_theta$cdf-0.95))])


```

  - Bayesian 2: Monte Carlo simulation
    Directly draw random numbers from the posterior distribution using the inverse-CDF method

```{R}
## Using evenly spaced pdf from Chapter 1
post_impft_cdf <- function(x=5, n=20, fp=0.07, fn=0.05, k=100){
    theta <- seq(0, 1,, k)
    fpst <- theta*(1-fn) + (1-theta)*fp
    post <- x*log(fpst) + (n-x)*log(1-fpst)
    return(cumsum(exp(post)/sum(exp(post))))
}

post_cdf <- data.frame(theta=seq(0,1,,5000), cdf=post_impft_cdf(k=5000))
u <- runif(n)
tmp <- apply(post_cdf, 1, function(x, unf)
     return(x[2]-unf), unf=u)

theta <- apply(tmp, 1, function(x, theta)
     return(theta[abs(x)==min(abs(x))]),
              theta=post_cdf$theta)
hist(theta)
mean(theta)
sd(theta)
median(theta)
quantile(theta, prob=c(0.05,0.95))
```
  - Bayesian 3: Using Metropolis-Hastings algorithm (See Section 2.4.1)
  An acceptance-rejection method (Section 2.3.2) with a candidate-generating distribution function and the acceptance probability probability is based on the ratio of posterior density function at two points. As a result, we only need to know the posterior density upto a proportional constant.   

```{R}
set.seed(10)
n_sims<-50000
theta <- numeric()
theta[1] <- runif(1) # initial value

n <- 20
x <- 5
log_lk <- function(theta, x=5, n=20, fp=0.07, fn=0.05){
    pp <- theta*(1-fn)+(1-theta)*fp
    llk <- x*log(pp)+(n-x)*log1p(-pp)
    return(llk)
}

for (j in 1:n_sims){
  y <- runif(1) ## unif(0,1) as the candidate-generating function
  alpha <- exp(log_lk(y)-log_lk(theta[j]))
  if (runif(1) < alpha) theta[j+1] <- y
  else theta[j+1] <- theta[j]
}

theta <- theta[round(1+n_sims/2):n_sims]

hist(theta)
mean(theta)
sd(theta)
median(theta)
quantile(theta, prob=c(0.05,0.95))
```

## Stan using `rstan`

  - Bayesian 4: Using Stan
  The basic task of Bayesian computation is to derive the posterior distribution
  
  $\pi(\theta\mid y_1,\cdots,y_n) \propto \pi(\theta)L(\theta; y_1,\cdots,y_n)$
  
  Using Stan, we draw random samples of the unknown parameters $\theta$ from their joint posterior distribution. As we know from the Metropolis-Hastings algorithm, we can draw random samples by knowing the posterior density upto a proportional constant. The rest can be automatic using a computer. In a Bayesian computing software, we need to provide three groups of information to formulate the MCMC algorithm.       1. Input data (e.g., $y, n, f_p, f_n$)
      2. Parameters to be estimated 
      3. The likelihood function and prior distributions 
Stan puts the above three groups of information in three code blocks. For the snake fungal example, we have $x=5, n=20$, $f_p=0.07$ and $f_n=0.05$. The response variable ($y$) is assumed to follow a binomial distribution $y \sim Bin(p, n)$, with the probability of a positive result being $p=\theta(1-f_n)+(1-\theta)f_p)$. Translating to Stan, we have
```{R}
### Stan Code ###
snake_code1 <- "
  data{
    int<lower=1> n;
    int<lower=0> y;
    real<lower=0,upper=1> fn;
    real<lower=0,upper=1> fp;
  }
  parameters{
    real<lower=0,upper=1> theta;
  }
  model{
    theta ~ beta(1,1);
    y ~ binomial(n, theta*(1-fn) + (1-theta)*fp );
  }
"
```
We can also specify log-likelihood function directly by changing the `model` block to:
```
model{
   theta ~ beta(1,1);
   target += y*log(theta*(1-fn)_(1-theta)*fp)+
             (n-y)*log(1-theta*(1-fn)-(1-theta)*fp);
}
```
We can also add a `transformed parameters` block to clarify the code:
```
transformed parameters{
   real<lower=0mupper=1> p_pos;
   p_pos = theta*(1-fn)+(1-theta)*fp;
}
model{
   theta ~ beta(1,1);
   target += y*log(p_pos)+(n-y)*log(1-p_pos);
}
```

or directly use the log-probability function come with Stan:

```
transformed parameters{
   real<lower=0mupper=1> p_pos;
   p_pos = theta*(1-fn)+(1-theta)*fp;
}
model{
   theta ~ beta(0,1);
   target += binomial_lpmf(y | n, p_pos);
}
```
Here we used $beta(1,1)$ (the same as uniform between 0 and 1) as the non-informative prior of $\theta$. Because its density is a constant, we can omit the line `theta ~ beta(1,1);`.  

Before we can run the Stan (code `snake_code1`), we need to load the package `rstan` and set some options

```{R}
packages(rv)
packages(rstan)
packages(car)
rstan_options(auto_write = TRUE)
options(mc.cores = min(c(parallel::detectCores(), 8)))

nchains <-  min(c(parallel::detectCores(), 8))
niters <- 5000
nkeep <- 2500
nthin <- ceiling((niters/2)*nchains/nkeep)
```
After that, we typically follow the following steps.

1. Compile the model 
```{R}
fit1 <- stan_model(model_code = snake_code1)
```

2. Organizing input data, initial values, and parameters to monitor using a function
```{R}
input_snake <- function(y=5, n=20, nchns=nchains){
  data <- list(y=y,n=n, fp=0.07,fn=0.05)
  inits <- list()
  for (i in 1:nchns)
    inits[[i]] <- list(theta=runif(1))
  pars=c("theta")
  return(list(data=data, inits=inits, pars=pars))
}
```
3. Run the model
```{R}
input.to.stan <- input_snake()
fit2keep <- sampling(fit1, data = input.to.stan$data,
                     init=input.to.stan$inits,
                     pars = input.to.stan$pars,
                     iter=niters, thin=nthin,
                     chains=nchains)
print(fit2keep)
```
4. Processing output

```{R}
stan_out <- rstan::extract(fit2keep)
theta <- rvsims(stan_out$theta)
rvhist(theta)
quantile(stan_out$theta, prob=c(0.05,0.95))
```

## Why Bayesian?
With Stan, Bayesian inference can be readily applied to more complex problems that are impossible for classical MLE. More importantly, Bayesian inference using Stan can provide useful information on model formulation and identify potential problems. Bayesian inference is also the best approach for information accumulation.

### Information accumulation
Suppose that we conducted a new survey of the snake fungal disease a year after the initial study to update the prevalence. Or, we may want to evaluate the temporal trend of the disease prevalence in the subsequent years. 

All the methods we used resulted in a consistent estimate of the prevalence of about 22% with a wide range of between 0.07-0.4. The uncertainty reflects the small sample size. Suppose that we now have a new sample of $n_2=12$ and $y_2=4$. Using classical statistics, we either estimate the prevalence using only the second sample, assuming that the prevalence has changed or combine the data (i.e., $n=35$ and $y=9$), assuming the population stays the same. 

With a Bayesian approach, we can summarize the posterior of $\theta$ from the first sample to form a prior of $\theta$ and update the prior using the new data. We typically use a beta distribution to summarize the distribution of a probability. Given the mean and standard deviation of 0.23 and 0.11 (from the Stan model), we propose a beta distribution with parameters $\alpha= 3.52$ and $\beta= 11.56$ (based on the method of moments). The prior can also be from similar studies elsewhere. All we need is an estimate of the mean and a likely range. We can treat the likely range as the usual 95% confidence interval (roughly 4 times standard deviation), from which derive a rough estimate of the standard deviation.  
```{R}
ybar <- mean(stan_out$theta)
s2 <- var(stan_out$theta)
alpha0 <- ybar*(ybar*(1-ybar)/s2 - 1)
beta0 <- alpha0*(1-ybar)/ybar 

alpha0+beta0
```
Once the initial estimates of $\alpha_0$ and $\beta_0$ are available, we can further revise the estimate based on our assessment of the relevancy of the prior to our data at hand.  For example, the relevancy of the prior may be measured by $\alpha+\beta$ if we interpret the sum as the sample size from which the prior was derived. For example, $\alpha_0+\beta_0=15$, somewhat smaller than the sample size we used (20), perhaps, because of the imperfect detection of the qPCR method. If we want to weight the prior less than 15 data points, we can rescale the prior parameter by setting $\alpha_0+\beta_0=n_0$ while keeping $\alpha_0/(\alpha_0+\beta_0)$ the same (equals to $\bar{y}$).

We now need additional data for the model:
```{R}
### Stan Code ###
snake_code2 <- "
  data{
    int<lower=1> n;
    int<lower=0> y;
    real<lower=0,upper=1> fn;
    real<lower=0,upper=1> fp;
    real<lower=0> alpha0;
    real<lower=0> beta0;
  }
  parameters{
    real<lower=0,upper=1> theta;
  }
  transformed parameters{
   real<lower=0,upper=1> p_pos;
   p_pos = theta*(1-fn)+(1-theta)*fp;
}
model{
   theta ~ beta(alpha0,beta0);
   target += binomial_lpmf(y | n, p_pos);
}
"

fit2 <- stan_model(model_code = snake_code2)
input_snake2 <- function(n=12, y=5, alpha, beta, nchns=nchains){
  data <- list(y=y,n=n, fp=0.07,fn=0.05, alpha0=alpha, beta0=beta)
  inits <- list()
  for (i in 1:nchns)
    inits[[i]] <- list(theta=runif(1))
  pars=c("theta")
  return(list(data=data, inits=inits, pars=pars))
}
input.to.stan <- input_snake2(alpha=alpha0, beta=beta0)
fit2keep <- sampling(fit2, data = input.to.stan$data,
                     init=input.to.stan$inits,
                     pars = input.to.stan$pars,
                     iter=niters, thin=nthin,
                     chains=nchains)
print(fit2keep)

stan_out <- rstan::extract(fit2keep)
theta <- rvsims(stan_out$theta)
rvhist(theta)
quantile(stan_out$theta, prob=c(0.05,0.95))
```

From here, we can evaluate whether the prevalence is increased by calculating the probability that the posterior is larger than the prior:

```{R}
prior<-rvbeta(1, alpha0, beta0)
Pr(theta>prior)
```
If we down weight the prior by setting $\alpha_0+\beta_0=6$, half of the data from the current year, the new prior data are $n=12, y=5$, we simply rerun the model by changing only the input data:

```{R}
alpha0 <- 6*ybar
beta0 <- 6-alpha0

input.to.stan <- input_snake2(n=12, y=5, alpha=alpha0, beta=beta0)
fit2keep <- sampling(fit2, data = input.to.stan$data,
                     init=input.to.stan$inits,
                     pars = input.to.stan$pars,
                     iter=niters, thin=nthin,
                     chains=nchains)
print(fit2keep)

stan_out <- rstan::extract(fit2keep)
theta <- rvsims(stan_out$theta)
rvhist(theta)
quantile(stan_out$theta, prob=c(0.05,0.95))
Pr(theta>prior)

```

## Realistic modeling
Most likely, we don't know exactly the values of $f_p$ and $f_n$. But we may have prior knowledge on their likely values, from which we can derive prior distributions of $f_p$ and $f_n$. For example, the values $f_p=0.07$ and $f_n=0.05 are estimated from similar tests elsewhere based on over 50 tests. We can use the beta distribution to quantify the prior knowledge:
```{R}
a_p <- 0.07*50
b_p <- 50-a_p
a_n <- 0.05*50
b_n = 50-a_n
```
Now the only change needed is to include $f_p$ and $f_n$ as parameters and their the priors:
```{R}
### Stan Code ###
snake_code3 <- "
  data{
    int<lower=1> n;
    int<lower=0> y;
    real<lower=0> alpha0;
    real<lower=0> beta0;
    real<lower=0> a_p;
    real<lower=0> b_p;
    real<lower=0> a_n;
    real<lower=0> b_n;
  }
  parameters{
    real<lower=0,upper=1> theta;
    real<lower=0,upper=1> fn;
    real<lower=0,upper=1> fp;
  }
  transformed parameters{
   real<lower=0,upper=1> p_pos;
   p_pos = theta*(1-fn)+(1-theta)*fp;
}
model{
   theta ~ beta(alpha0,beta0);
   fp ~ beta(a_p, b_p);
   fn ~ beta(a_n, b_n);
   target += binomial_lpmf(y | n, p_pos);
}
"

fit3 <- stan_model(model_code = snake_code3)
input_snake3 <- function(n=20, y=5, alpha, beta, ap, bp, an, bn, nchns=nchains){
  data <- list(y=y,n=n, fp=0.07,fn=0.05, alpha0=alpha, beta0=beta, a_p=ap, b_p=bp, a_n=an, b_n=bn)
  inits <- list()
  for (i in 1:nchns)
    inits[[i]] <- list(theta=runif(1))
  pars=c("theta", "fp","fn")
  return(list(data=data, inits=inits, pars=pars))
}
input.to.stan <- input_snake3(alpha=alpha0, beta=beta0, ap=a_p, an=a_n, bp=b_p, bn=b_n)
fit2keep <- sampling(fit3, data = input.to.stan$data,
                     init=input.to.stan$inits,
                     pars = input.to.stan$pars,
                     iter=niters, thin=nthin,
                     chains=nchains)
print(fit2keep)
pairs(fit2keep, pars=c("theta", "fp", "fn"))
stan_out <- rstan::extract(fit2keep)
theta <- rvsims(stan_out$theta)
rvhist(theta)
quantile(stan_out$theta, prob=c(0.05,0.95))
```
### Identifiability
What happens if we have no prior information of $f_p$ and $f_n$? We could try to use non-informative prior $beta(1,1)$ (uniform between 0 and 1). However, it is unlikely that we can simultaneously identify all three parameters. Mathematically, information in the data is represented in the likelihood function, consisting of products of $\theta f_p$ and $\theta f_n$. In other words, the data have information of the two products. To definitely separate them, we need additional information.  If we can try because we can, Stan will let us know that something is wrong.
```{R}
input.to.stan <- input_snake3(alpha=alpha0, beta=beta0, ap=1, an=1, bp=1, bn=1)
fit2keep <- sampling(fit3, data = input.to.stan$data,
                     init=input.to.stan$inits,
                     pars = input.to.stan$pars,
                     iter=niters, thin=nthin,
                     chains=nchains)
print(fit2keep)
pairs(fit2keep, pars=c("theta", "fp", "fn"))
stan_out <- rstan::extract(fit2keep)
theta <- rvsims(stan_out$theta)
rvhist(theta)
quantile(stan_out$theta, prob=c(0.05,0.95))
```
To show the non-identifiability clearly, we increase the sample size from $n=20$ to $n=2000$
```{R}
input.to.stan <- input_snake3(n=2000, y=500, alpha=alpha0, beta=beta0, ap=1, an=1, bp=1, bn=1)
fit2keep <- sampling(fit3, data = input.to.stan$data,
                     init=input.to.stan$inits,
                     pars = input.to.stan$pars,
                     iter=niters, thin=nthin,
                     chains=nchains)
print(fit2keep)
pairs(fit2keep, pars=c("theta", "fp", "fn"))
stan_out <- rstan::extract(fit2keep)
theta <- rvsims(stan_out$theta)
rvhist(theta)
quantile(stan_out$theta, prob=c(0.05,0.95))
```
Examine the marginal distributions is not enough.

## Summary
Using Stan in R is a natural way for implementing Bayesian modeling.  Writing the Stan model requires us explicitly formulate a model, identify parameters of interest, and provide any prior information we have on the parameters. The strong diagnostic feature of Stan makes model evaluation simple. A typical Bayesian inference workflow starts with writing down the response variable model (what is the likely probability distribution model and how the mean parameter relates to predictors). From the likelihood function, we identify parameters of interest and relevant prior information.  We then organize all available information into three basic blocks (data, parameters, and model) to construct the Stan model code. Because the MCMC algorithm in Stan is constructed through a function proportional to the log-posterior density, the log-likelihood function in a Stan model does not have to be based on a known distribution function (e.g., the normal distribution).   

## Computational Notes -- Using Package `rv`

See [cran rv vignette](https://cran.r-project.org/web/packages/rv/vignettes/rv-doc.html)

